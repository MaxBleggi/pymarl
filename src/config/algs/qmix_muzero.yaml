# --- QMIX-MUZERO specific parameters ---
# Uses a learned model to guide exploration via monte carlo tree search
#

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
learner: "q_learner"
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

# models
runner: "model_muzero_episode"
model_learner: "model_muzero"
dynamics_model_hidden_dim: 128
policy_actions_model_hidden_dim: 256

# model learning params
model_min_samples: 1
model_test_ratio: 0.1
model_learning_rate: 0.01
model_grad_clip_norm: 0.001
model_batch_size: 32
model_intitial_epochs: 1
model_epochs: 1 # per RL step (this is the number of times the replay buffer is sampled between real env execution)
model_log_epochs: 10
model_bootstrap_timesteps: 50 # n in the n-step return estimate
model_rollout_timesteps: 5 # number of timesteps to simulate during model training
model_rollout_timestep_samples: 10
model_mcts_simulations: 50
model_state_prior_steps: 10 # number of prior timesteps to use for encoding state
model_dirichlet_alpha: 0.25
model_add_exploration_noise: False
model_select_action_from_target_policy: False # use qmix etc for action selection - used for debugging
model_predict_target_policy: True # muzero estimates the qmix etc policy otherwise use the mcts visit count etc distribution
model_mcts_tree_policy: "epsilon_greedy" # 'epsilon_greedy' or 'ucb'
model_mcts_tree_policy_metric: "priors" # priors, counts, values # only applies to model_mcts_tree_policy=epsilon_greedy
model_mcts_root_policy_metric: "counts" # priors=target policy, counts=visit count distribution, values=mcts action values

# rollouts
model_rollout_batch_size: 1 # for this to be bigger we would need to generate more candidate actions (ie expand multiple child nodes)
model_term_threshold: 0.5 # masks termination signal outputs at this threshold
model_action_threshold: 0.5 # # masks available action outputs at this threshold
model_use_search: True

# action selection
model_mac: "muzero_mac"
model_action_selector: "muzero_epsilon_greedy"
ucb_temperature_start: 1
ucb_temperature_finish: 0.05
ucb_temperature_anneal_time: 50000
ucb_temperature_delay: 0
ucb_c1: 1.25
ucb_c2: 19652
mcts_update_interval: 1

# debug
model_save_val_data: True
#episode_dir: "/home/ubuntu/masters/experiments/model-learning/1/val_data"

name: "qmix-muzero"
use_cuda: False